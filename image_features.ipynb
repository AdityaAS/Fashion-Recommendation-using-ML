{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "qYnqkolmksS2"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import optim, nn\n",
    "from torchvision import models, transforms\n",
    "model = models.vgg16(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: torchvision in /home/sannavajjala6/miniconda3/envs/mlp/lib/python3.9/site-packages (0.12.0)\n",
      "Requirement already satisfied: typing_extensions in /home/sannavajjala6/miniconda3/envs/mlp/lib/python3.9/site-packages (from torchvision) (4.2.0)\n",
      "Requirement already satisfied: numpy in /home/sannavajjala6/miniconda3/envs/mlp/lib/python3.9/site-packages (from torchvision) (1.21.5)\n",
      "Requirement already satisfied: requests in /home/sannavajjala6/miniconda3/envs/mlp/lib/python3.9/site-packages (from torchvision) (2.27.1)\n",
      "Requirement already satisfied: torch in /home/sannavajjala6/miniconda3/envs/mlp/lib/python3.9/site-packages (from torchvision) (1.11.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/sannavajjala6/miniconda3/envs/mlp/lib/python3.9/site-packages (from torchvision) (9.0.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/sannavajjala6/miniconda3/envs/mlp/lib/python3.9/site-packages (from requests->torchvision) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/sannavajjala6/miniconda3/envs/mlp/lib/python3.9/site-packages (from requests->torchvision) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/sannavajjala6/miniconda3/envs/mlp/lib/python3.9/site-packages (from requests->torchvision) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/sannavajjala6/miniconda3/envs/mlp/lib/python3.9/site-packages (from requests->torchvision) (2021.10.8)\n"
     ]
    }
   ],
   "source": [
    "!pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "rGQl-T8jk7HO"
   },
   "outputs": [],
   "source": [
    "class FeatureExtractor(nn.Module):\n",
    "  def __init__(self, model):\n",
    "    super(FeatureExtractor, self).__init__()\n",
    "\t\t# Extract VGG-16 Feature Layers\n",
    "    self.features = list(model.features)\n",
    "    self.features = nn.Sequential(*self.features)\n",
    "\t\t# Extract VGG-16 Average Pooling Layer\n",
    "    self.pooling = model.avgpool\n",
    "\t\t# Convert the image into one-dimensional vector\n",
    "    self.flatten = nn.Flatten()\n",
    "\t\t# Extract the first part of fully-connected layer from VGG16\n",
    "    self.fc = model.classifier[0]\n",
    "  \n",
    "  def forward(self, x):\n",
    "\t\t# It will take the input 'x' until it returns the feature vector called 'out'\n",
    "    out = self.features(x)\n",
    "    out = self.pooling(out)\n",
    "    out = self.flatten(out)\n",
    "    out = self.fc(out) \n",
    "    return out \n",
    "\n",
    "# Initialize the model\n",
    "model = models.vgg16(pretrained=True)\n",
    "new_model = FeatureExtractor(model)\n",
    "\n",
    "# Change the device to GPU\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else \"cpu\")\n",
    "new_model = new_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: opencv-python in /home/sannavajjala6/miniconda3/envs/mlp/lib/python3.9/site-packages (4.5.5.64)\n",
      "Requirement already satisfied: numpy>=1.19.3 in /home/sannavajjala6/miniconda3/envs/mlp/lib/python3.9/site-packages (from opencv-python) (1.21.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105101\n"
     ]
    }
   ],
   "source": [
    "from os.path import join\n",
    "import os\n",
    "\n",
    "root_dir = '/nethome/sannavajjala6/projects/ml_project/images'\n",
    "image_paths = []\n",
    "for root, dirs, files in os.walk(root_dir):\n",
    "    files = [join(root, x) for x in files]\n",
    "    image_paths.extend(files)\n",
    "\n",
    "print(len(image_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('transactions_train.csv')\n",
    "customers = pd.read_csv(\"customers.csv\")\n",
    "articles = pd.read_csv(\"articles.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_customers = df['customer_id'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_rows_by_values(df, col, values):\n",
    "    return df[df[col].isin(values)==True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n",
      "100000\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(44)\n",
    "sampled_customers = np.random.choice(all_customers, size=int(1e5), replace=False, )\n",
    "print(len(sampled_customers))\n",
    "print(len(set(sampled_customers)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n"
     ]
    }
   ],
   "source": [
    "df_sampled = df.loc[df['customer_id'].isin(sampled_customers)]\n",
    "print(len(df_sampled.customer_id.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_based_on_date(df, start_date='2020-06-22', end_date='2020-09-22'):\n",
    "    x = df[(df['t_dat'] > start_date) & (df['t_dat'] <= end_date)]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = '2020-01-01'\n",
    "x = query_based_on_date(df_sampled, start_date)\n",
    "train_x = query_based_on_date(x, start_date, '2020-07-22')\n",
    "test_x = query_based_on_date(x, '2020-07-22')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_x.sort_values(by=['t_dat'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53767\n"
     ]
    }
   ],
   "source": [
    "product_ids = []\n",
    "product_ids += list(train_x.article_id.unique())\n",
    "product_ids += list(test_x.article_id.unique())\n",
    "\n",
    "print(len(product_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53683\n"
     ]
    }
   ],
   "source": [
    "from os.path import join, exists\n",
    "product_paths = []\n",
    "\n",
    "for product_id in product_ids:\n",
    "    product_id = str(product_id)\n",
    "    path = join('./images', '0' + product_id[:2], '0' + product_id + '.jpg')\n",
    "    if exists(path):\n",
    "        product_paths.append(path)\n",
    "\n",
    "print(len(product_paths))\n",
    "    \n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(0)\n",
    "\n",
    "random.shuffle(image_paths)\n",
    "sample_image_paths = image_paths[:5000]\n",
    "print(len(sample_image_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/nethome/sannavajjala6/projects/ml_project\n"
     ]
    }
   ],
   "source": [
    "# with open('image_samples.txt', 'w') as f:\n",
    "#     f.writelines([x + '\\n' for x in sample_image_paths])\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53683\n"
     ]
    }
   ],
   "source": [
    "sample_image_paths = product_paths\n",
    "\n",
    "# from os.path import join\n",
    "# with open('final_image_paths.txt', 'r') as f:\n",
    "#     sample_image_paths = f.readlines()\n",
    "#     sample_image_paths = [join('/nethome/sannavajjala6/projects/ml_project/images/', x.rstrip()) for x in sample_image_paths]\n",
    "    \n",
    "    \n",
    "print(len(sample_image_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53683\n",
      "53683\n",
      "[720125001, 884319008, 923727002, 685811007, 685816004]\n",
      "['./images/072/0720125001.jpg', './images/088/0884319008.jpg', './images/092/0923727002.jpg', './images/068/0685811007.jpg', './images/068/0685816004.jpg']\n"
     ]
    }
   ],
   "source": [
    "sample_product_ids = []\n",
    "for image_path in sample_image_paths:\n",
    "    sample_product_ids.append(int(image_path.split('/')[-1].split('.')[0]))\n",
    "\n",
    "print(len(sample_product_ids))\n",
    "print(len(sample_image_paths))\n",
    "print(sample_product_ids[:5])\n",
    "print(sample_image_paths[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['article_id', 'product_code', 'prod_name', 'product_type_no',\n",
      "       'product_type_name', 'product_group_name', 'graphical_appearance_no',\n",
      "       'graphical_appearance_name', 'colour_group_code', 'colour_group_name',\n",
      "       'perceived_colour_value_id', 'perceived_colour_value_name',\n",
      "       'perceived_colour_master_id', 'perceived_colour_master_name',\n",
      "       'department_no', 'department_name', 'index_code', 'index_name',\n",
      "       'index_group_no', 'index_group_name', 'section_no', 'section_name',\n",
      "       'garment_group_no', 'garment_group_name', 'detail_desc'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "articles = pd.read_csv('articles.csv')\n",
    "print(articles.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>product_code</th>\n",
       "      <th>prod_name</th>\n",
       "      <th>product_type_no</th>\n",
       "      <th>product_type_name</th>\n",
       "      <th>product_group_name</th>\n",
       "      <th>graphical_appearance_no</th>\n",
       "      <th>graphical_appearance_name</th>\n",
       "      <th>colour_group_code</th>\n",
       "      <th>colour_group_name</th>\n",
       "      <th>...</th>\n",
       "      <th>department_name</th>\n",
       "      <th>index_code</th>\n",
       "      <th>index_name</th>\n",
       "      <th>index_group_no</th>\n",
       "      <th>index_group_name</th>\n",
       "      <th>section_no</th>\n",
       "      <th>section_name</th>\n",
       "      <th>garment_group_no</th>\n",
       "      <th>garment_group_name</th>\n",
       "      <th>detail_desc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>108775015</td>\n",
       "      <td>108775</td>\n",
       "      <td>Strap top</td>\n",
       "      <td>253</td>\n",
       "      <td>Vest top</td>\n",
       "      <td>Garment Upper body</td>\n",
       "      <td>1010016</td>\n",
       "      <td>Solid</td>\n",
       "      <td>9</td>\n",
       "      <td>Black</td>\n",
       "      <td>...</td>\n",
       "      <td>Jersey Basic</td>\n",
       "      <td>A</td>\n",
       "      <td>Ladieswear</td>\n",
       "      <td>1</td>\n",
       "      <td>Ladieswear</td>\n",
       "      <td>16</td>\n",
       "      <td>Womens Everyday Basics</td>\n",
       "      <td>1002</td>\n",
       "      <td>Jersey Basic</td>\n",
       "      <td>Jersey top with narrow shoulder straps.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>108775044</td>\n",
       "      <td>108775</td>\n",
       "      <td>Strap top</td>\n",
       "      <td>253</td>\n",
       "      <td>Vest top</td>\n",
       "      <td>Garment Upper body</td>\n",
       "      <td>1010016</td>\n",
       "      <td>Solid</td>\n",
       "      <td>10</td>\n",
       "      <td>White</td>\n",
       "      <td>...</td>\n",
       "      <td>Jersey Basic</td>\n",
       "      <td>A</td>\n",
       "      <td>Ladieswear</td>\n",
       "      <td>1</td>\n",
       "      <td>Ladieswear</td>\n",
       "      <td>16</td>\n",
       "      <td>Womens Everyday Basics</td>\n",
       "      <td>1002</td>\n",
       "      <td>Jersey Basic</td>\n",
       "      <td>Jersey top with narrow shoulder straps.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>108775051</td>\n",
       "      <td>108775</td>\n",
       "      <td>Strap top (1)</td>\n",
       "      <td>253</td>\n",
       "      <td>Vest top</td>\n",
       "      <td>Garment Upper body</td>\n",
       "      <td>1010017</td>\n",
       "      <td>Stripe</td>\n",
       "      <td>11</td>\n",
       "      <td>Off White</td>\n",
       "      <td>...</td>\n",
       "      <td>Jersey Basic</td>\n",
       "      <td>A</td>\n",
       "      <td>Ladieswear</td>\n",
       "      <td>1</td>\n",
       "      <td>Ladieswear</td>\n",
       "      <td>16</td>\n",
       "      <td>Womens Everyday Basics</td>\n",
       "      <td>1002</td>\n",
       "      <td>Jersey Basic</td>\n",
       "      <td>Jersey top with narrow shoulder straps.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>110065001</td>\n",
       "      <td>110065</td>\n",
       "      <td>OP T-shirt (Idro)</td>\n",
       "      <td>306</td>\n",
       "      <td>Bra</td>\n",
       "      <td>Underwear</td>\n",
       "      <td>1010016</td>\n",
       "      <td>Solid</td>\n",
       "      <td>9</td>\n",
       "      <td>Black</td>\n",
       "      <td>...</td>\n",
       "      <td>Clean Lingerie</td>\n",
       "      <td>B</td>\n",
       "      <td>Lingeries/Tights</td>\n",
       "      <td>1</td>\n",
       "      <td>Ladieswear</td>\n",
       "      <td>61</td>\n",
       "      <td>Womens Lingerie</td>\n",
       "      <td>1017</td>\n",
       "      <td>Under-, Nightwear</td>\n",
       "      <td>Microfibre T-shirt bra with underwired, moulde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>110065002</td>\n",
       "      <td>110065</td>\n",
       "      <td>OP T-shirt (Idro)</td>\n",
       "      <td>306</td>\n",
       "      <td>Bra</td>\n",
       "      <td>Underwear</td>\n",
       "      <td>1010016</td>\n",
       "      <td>Solid</td>\n",
       "      <td>10</td>\n",
       "      <td>White</td>\n",
       "      <td>...</td>\n",
       "      <td>Clean Lingerie</td>\n",
       "      <td>B</td>\n",
       "      <td>Lingeries/Tights</td>\n",
       "      <td>1</td>\n",
       "      <td>Ladieswear</td>\n",
       "      <td>61</td>\n",
       "      <td>Womens Lingerie</td>\n",
       "      <td>1017</td>\n",
       "      <td>Under-, Nightwear</td>\n",
       "      <td>Microfibre T-shirt bra with underwired, moulde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105537</th>\n",
       "      <td>953450001</td>\n",
       "      <td>953450</td>\n",
       "      <td>5pk regular Placement1</td>\n",
       "      <td>302</td>\n",
       "      <td>Socks</td>\n",
       "      <td>Socks &amp; Tights</td>\n",
       "      <td>1010014</td>\n",
       "      <td>Placement print</td>\n",
       "      <td>9</td>\n",
       "      <td>Black</td>\n",
       "      <td>...</td>\n",
       "      <td>Socks Bin</td>\n",
       "      <td>F</td>\n",
       "      <td>Menswear</td>\n",
       "      <td>3</td>\n",
       "      <td>Menswear</td>\n",
       "      <td>26</td>\n",
       "      <td>Men Underwear</td>\n",
       "      <td>1021</td>\n",
       "      <td>Socks and Tights</td>\n",
       "      <td>Socks in a fine-knit cotton blend with a small...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105538</th>\n",
       "      <td>953763001</td>\n",
       "      <td>953763</td>\n",
       "      <td>SPORT Malaga tank</td>\n",
       "      <td>253</td>\n",
       "      <td>Vest top</td>\n",
       "      <td>Garment Upper body</td>\n",
       "      <td>1010016</td>\n",
       "      <td>Solid</td>\n",
       "      <td>9</td>\n",
       "      <td>Black</td>\n",
       "      <td>...</td>\n",
       "      <td>Jersey</td>\n",
       "      <td>A</td>\n",
       "      <td>Ladieswear</td>\n",
       "      <td>1</td>\n",
       "      <td>Ladieswear</td>\n",
       "      <td>2</td>\n",
       "      <td>H&amp;M+</td>\n",
       "      <td>1005</td>\n",
       "      <td>Jersey Fancy</td>\n",
       "      <td>Loose-fitting sports vest top in ribbed fast-d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105539</th>\n",
       "      <td>956217002</td>\n",
       "      <td>956217</td>\n",
       "      <td>Cartwheel dress</td>\n",
       "      <td>265</td>\n",
       "      <td>Dress</td>\n",
       "      <td>Garment Full body</td>\n",
       "      <td>1010016</td>\n",
       "      <td>Solid</td>\n",
       "      <td>9</td>\n",
       "      <td>Black</td>\n",
       "      <td>...</td>\n",
       "      <td>Jersey</td>\n",
       "      <td>A</td>\n",
       "      <td>Ladieswear</td>\n",
       "      <td>1</td>\n",
       "      <td>Ladieswear</td>\n",
       "      <td>18</td>\n",
       "      <td>Womens Trend</td>\n",
       "      <td>1005</td>\n",
       "      <td>Jersey Fancy</td>\n",
       "      <td>Short, A-line dress in jersey with a round nec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105540</th>\n",
       "      <td>957375001</td>\n",
       "      <td>957375</td>\n",
       "      <td>CLAIRE HAIR CLAW</td>\n",
       "      <td>72</td>\n",
       "      <td>Hair clip</td>\n",
       "      <td>Accessories</td>\n",
       "      <td>1010016</td>\n",
       "      <td>Solid</td>\n",
       "      <td>9</td>\n",
       "      <td>Black</td>\n",
       "      <td>...</td>\n",
       "      <td>Small Accessories</td>\n",
       "      <td>D</td>\n",
       "      <td>Divided</td>\n",
       "      <td>2</td>\n",
       "      <td>Divided</td>\n",
       "      <td>52</td>\n",
       "      <td>Divided Accessories</td>\n",
       "      <td>1019</td>\n",
       "      <td>Accessories</td>\n",
       "      <td>Large plastic hair claw.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105541</th>\n",
       "      <td>959461001</td>\n",
       "      <td>959461</td>\n",
       "      <td>Lounge dress</td>\n",
       "      <td>265</td>\n",
       "      <td>Dress</td>\n",
       "      <td>Garment Full body</td>\n",
       "      <td>1010016</td>\n",
       "      <td>Solid</td>\n",
       "      <td>11</td>\n",
       "      <td>Off White</td>\n",
       "      <td>...</td>\n",
       "      <td>Jersey</td>\n",
       "      <td>A</td>\n",
       "      <td>Ladieswear</td>\n",
       "      <td>1</td>\n",
       "      <td>Ladieswear</td>\n",
       "      <td>18</td>\n",
       "      <td>Womens Trend</td>\n",
       "      <td>1005</td>\n",
       "      <td>Jersey Fancy</td>\n",
       "      <td>Calf-length dress in ribbed jersey made from a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>105542 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        article_id  product_code               prod_name  product_type_no  \\\n",
       "0        108775015        108775               Strap top              253   \n",
       "1        108775044        108775               Strap top              253   \n",
       "2        108775051        108775           Strap top (1)              253   \n",
       "3        110065001        110065       OP T-shirt (Idro)              306   \n",
       "4        110065002        110065       OP T-shirt (Idro)              306   \n",
       "...            ...           ...                     ...              ...   \n",
       "105537   953450001        953450  5pk regular Placement1              302   \n",
       "105538   953763001        953763       SPORT Malaga tank              253   \n",
       "105539   956217002        956217         Cartwheel dress              265   \n",
       "105540   957375001        957375        CLAIRE HAIR CLAW               72   \n",
       "105541   959461001        959461            Lounge dress              265   \n",
       "\n",
       "       product_type_name  product_group_name  graphical_appearance_no  \\\n",
       "0               Vest top  Garment Upper body                  1010016   \n",
       "1               Vest top  Garment Upper body                  1010016   \n",
       "2               Vest top  Garment Upper body                  1010017   \n",
       "3                    Bra           Underwear                  1010016   \n",
       "4                    Bra           Underwear                  1010016   \n",
       "...                  ...                 ...                      ...   \n",
       "105537             Socks      Socks & Tights                  1010014   \n",
       "105538          Vest top  Garment Upper body                  1010016   \n",
       "105539             Dress   Garment Full body                  1010016   \n",
       "105540         Hair clip         Accessories                  1010016   \n",
       "105541             Dress   Garment Full body                  1010016   \n",
       "\n",
       "       graphical_appearance_name  colour_group_code colour_group_name  ...  \\\n",
       "0                          Solid                  9             Black  ...   \n",
       "1                          Solid                 10             White  ...   \n",
       "2                         Stripe                 11         Off White  ...   \n",
       "3                          Solid                  9             Black  ...   \n",
       "4                          Solid                 10             White  ...   \n",
       "...                          ...                ...               ...  ...   \n",
       "105537           Placement print                  9             Black  ...   \n",
       "105538                     Solid                  9             Black  ...   \n",
       "105539                     Solid                  9             Black  ...   \n",
       "105540                     Solid                  9             Black  ...   \n",
       "105541                     Solid                 11         Off White  ...   \n",
       "\n",
       "          department_name index_code        index_name index_group_no  \\\n",
       "0            Jersey Basic          A        Ladieswear              1   \n",
       "1            Jersey Basic          A        Ladieswear              1   \n",
       "2            Jersey Basic          A        Ladieswear              1   \n",
       "3          Clean Lingerie          B  Lingeries/Tights              1   \n",
       "4          Clean Lingerie          B  Lingeries/Tights              1   \n",
       "...                   ...        ...               ...            ...   \n",
       "105537          Socks Bin          F          Menswear              3   \n",
       "105538             Jersey          A        Ladieswear              1   \n",
       "105539             Jersey          A        Ladieswear              1   \n",
       "105540  Small Accessories          D           Divided              2   \n",
       "105541             Jersey          A        Ladieswear              1   \n",
       "\n",
       "        index_group_name section_no            section_name garment_group_no  \\\n",
       "0             Ladieswear         16  Womens Everyday Basics             1002   \n",
       "1             Ladieswear         16  Womens Everyday Basics             1002   \n",
       "2             Ladieswear         16  Womens Everyday Basics             1002   \n",
       "3             Ladieswear         61         Womens Lingerie             1017   \n",
       "4             Ladieswear         61         Womens Lingerie             1017   \n",
       "...                  ...        ...                     ...              ...   \n",
       "105537          Menswear         26           Men Underwear             1021   \n",
       "105538        Ladieswear          2                    H&M+             1005   \n",
       "105539        Ladieswear         18            Womens Trend             1005   \n",
       "105540           Divided         52     Divided Accessories             1019   \n",
       "105541        Ladieswear         18            Womens Trend             1005   \n",
       "\n",
       "        garment_group_name                                        detail_desc  \n",
       "0             Jersey Basic            Jersey top with narrow shoulder straps.  \n",
       "1             Jersey Basic            Jersey top with narrow shoulder straps.  \n",
       "2             Jersey Basic            Jersey top with narrow shoulder straps.  \n",
       "3        Under-, Nightwear  Microfibre T-shirt bra with underwired, moulde...  \n",
       "4        Under-, Nightwear  Microfibre T-shirt bra with underwired, moulde...  \n",
       "...                    ...                                                ...  \n",
       "105537    Socks and Tights  Socks in a fine-knit cotton blend with a small...  \n",
       "105538        Jersey Fancy  Loose-fitting sports vest top in ribbed fast-d...  \n",
       "105539        Jersey Fancy  Short, A-line dress in jersey with a round nec...  \n",
       "105540         Accessories                           Large plastic hair claw.  \n",
       "105541        Jersey Fancy  Calf-length dress in ribbed jersey made from a...  \n",
       "\n",
       "[105542 rows x 25 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         108775015\n",
       "1         108775044\n",
       "2         108775051\n",
       "3         110065001\n",
       "4         110065002\n",
       "            ...    \n",
       "105537    953450001\n",
       "105538    953763001\n",
       "105539    956217002\n",
       "105540    957375001\n",
       "105541    959461001\n",
       "Name: article_id, Length: 105542, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles['article_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sports tights in fast-drying functional fabric with a wide waistband to hold in and shape the waist. Regular waist with a concealed key pocket in the waistband.'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_article_ids = []\n",
    "\n",
    "articles.loc[articles['article_id'] == sample_product_ids[0], 'detail_desc'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53683\n"
     ]
    }
   ],
   "source": [
    "sample_product_descs = []\n",
    "for product_id in sample_product_ids:\n",
    "    sample_product_descs.append(articles.loc[articles['article_id'] == sample_product_ids[0], 'detail_desc'].iloc[0])\n",
    "print(len(sample_product_descs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "from typing import List, Optional\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = AutoModel.from_pretrained('distilbert-base-uncased', output_hidden_states=True)\n",
    "\n",
    "def gen_embeds(sents: List[str], average: bool = True) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    sents:  List of sentences to generate embeddings for\n",
    "    average: Return average over sentences\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor of size num_sents x emb_dim if average = True else\n",
    "                    size num_sents x max_len, emb_dim\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(sents, padding=True, return_tensors=\"pt\")\n",
    "        logits = model(**inputs).last_hidden_state\n",
    "\n",
    "        if average:\n",
    "            # Average across the entire sentence\n",
    "            return torch.mean(logits, dim=1)\n",
    "        else:\n",
    "            return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_embeds = gen_embeds(sample_product_descs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([53683, 768])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.save('text_features_full.npy', sample_embeds.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_image_and_transform(image_path):\n",
    "    img = cv2.imread(image_path)\n",
    "    img = transform(img)\n",
    "    img = img.reshape(1, 3, 448, 448)\n",
    "    img = img.to(device)\n",
    "    return img\n",
    "\n",
    "results = Parallel(n_jobs=20)(delayed(load_image_and_transform)(image_path) for image_path in tqdm(sample_image_paths))\n",
    "\n",
    "print(len(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|███▏                                                          | 2793/53683 [05:55<1:27:20,  9.71it/s]"
     ]
    }
   ],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "# from google.colab.patches import cv2_imshow\n",
    "\n",
    "# Transform the image, so it becomes readable with the model\n",
    "transform = transforms.Compose([\n",
    "  transforms.ToPILImage(),\n",
    "  transforms.CenterCrop(512),\n",
    "  transforms.Resize(448),\n",
    "  transforms.ToTensor()                              \n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "image_list = []\n",
    "for image_path in tqdm(sample_image_paths):\n",
    "    img = cv2.imread(image_path)\n",
    "    img = transform(img)\n",
    "    img = img.reshape(1, 3, 448, 448)\n",
    "    image_list.append(img.cpu())\n",
    "\n",
    "print(len(image_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_tensor = torch.cat(image_list)\n",
    "print(image_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4ytBaHFglBVT",
    "outputId": "6993dcbc-fc43-401b-a043-987ba0d9729e"
   },
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "# import numpy as np\n",
    "# import os\n",
    "# import cv2\n",
    "# # from google.colab.patches import cv2_imshow\n",
    "\n",
    "# # Transform the image, so it becomes readable with the model\n",
    "# transform = transforms.Compose([\n",
    "#   transforms.ToPILImage(),\n",
    "#   transforms.CenterCrop(512),\n",
    "#   transforms.Resize(448),\n",
    "#   transforms.ToTensor()                              \n",
    "# ])\n",
    "\n",
    "# Will contain the feature\n",
    "features = []\n",
    "image_id = []\n",
    "# path = 'images'\n",
    "# images = os.listdir(path)\n",
    "# print(\"Total images:\", len(images))\n",
    "# for image in images:\n",
    "  # print(image)\n",
    "\n",
    "# images = sample_image_paths\n",
    "# Iterate each image\n",
    "\n",
    "for start_idx in tqdm(range(0, 4900, 100)):\n",
    "    curr_batch = image_tensor[start_idx:start_idx+100]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "      # Extract the feature from the image\n",
    "      feature = new_model(curr_batch)\n",
    "\n",
    "    # Convert to NumPy Array, Reshape it, and save it to features variable\n",
    "    features.append(feature.cpu().detach())\n",
    "\n",
    "    \n",
    "# for i in images:\n",
    "#     # print (i)\n",
    "#     image_id.append(i.split('/')[-1])\n",
    "#     # Set the image path\n",
    "#     # print(\"image\", i)\n",
    "#     # print('path:', path)\n",
    "#     imagePath = i\n",
    "#     # imagePath = os.path.join(path + '/', str(i))\n",
    "#     # print(imagePath)\n",
    "#     # Read the file\n",
    "#     img = cv2.imread(imagePath)\n",
    "#     # cv2_imshow(img)\n",
    "#     # Transform the image\n",
    "#     img = transform(img)\n",
    "#     # Reshape the image. PyTorch model reads 4-dimensional tensor\n",
    "#     # [batch_size, channels, width, height]\n",
    "#     img = img.reshape(1, 3, 448, 448)\n",
    "#     img = img.to(device)\n",
    "#     # We only extract features, so we don't need gradient\n",
    "#     with torch.no_grad():\n",
    "#       # Extract the feature from the image\n",
    "#       feature = new_model(img)\n",
    "#     # Convert to NumPy Array, Reshape it, and save it to features variable\n",
    "#     features.append(feature.cpu().detach().numpy().reshape(-1))\n",
    "\n",
    "# # Convert to NumPy Array\n",
    "# features = np.array(features)\n",
    "# print(features.shape)\n",
    "# print(features[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = torch.cat(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.save('features_2.npy', all_features.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0OR1Rb8JlEjo",
    "outputId": "6cc68bbb-54c2-4846-ed70-7f228260be71"
   },
   "outputs": [],
   "source": [
    "from numpy.ma.core import argmin\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing import image\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "# Initialize the model\n",
    "# model = KMeans(n_clusters=6, random_state=42)\n",
    "# num_clusters = 133\n",
    "min_num_clusters = 15\n",
    "max_num_clusters = 50\n",
    "silhouette_scores = []\n",
    "davies_bouldin_scores = []\n",
    "# print(\"Silhouette Scores\")\n",
    "\n",
    "print(\"k \\t Silhouette \\t DB\")\n",
    "for num_clusters in range(min_num_clusters, max_num_clusters):\n",
    "  print()\n",
    "  model = KMeans(n_clusters=num_clusters)\n",
    "\n",
    "  # Fit the data into the model\n",
    "  model.fit(features)\n",
    "\n",
    "  # Extract the labels\n",
    "  labels = model.labels_\n",
    "  \n",
    "  # Calculate Silhouette score\n",
    "  s = metrics.silhouette_score(features, labels, metric='euclidean')\n",
    "  silhouette_scores.append(s)\n",
    "  d = metrics.davies_bouldin_score(features, labels)\n",
    "  davies_bouldin_scores.append(d)\n",
    "  print(num_clusters, \"\\t\", s, \"\\t\", d)\n",
    "\n",
    "\n",
    "silhouette_scores = np.array(silhouette_scores)\n",
    "davies_bouldin_scores = np.array(davies_bouldin_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "id": "y9COHXH-JGbr",
    "outputId": "25856177-c265-4677-841e-48a35bae0a0d"
   },
   "outputs": [],
   "source": [
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "\n",
    "model = KMeans()\n",
    "visualizer = KElbowVisualizer(model, k=(min_num_clusters, max_num_clusters))\n",
    "\n",
    "visualizer.fit(features)    # Fit the data to the visualizer\n",
    "visualizer.show()           # Finalize and render the figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lZfRc0ndGzT1",
    "outputId": "d4d4a375-0cd6-4db1-9fd6-65c0002de584"
   },
   "outputs": [],
   "source": [
    "best_num_clusters = 34\n",
    "best_num_clusters_index = best_num_clusters - min_num_clusters  \n",
    "\n",
    "print(\"Optimal no. of clusters is\", best_num_clusters, \"by Elbow Method\")\n",
    "\n",
    "\n",
    "# best_num_clusters_index = np.argmax(silhouette_scores)\n",
    "# best_num_clusters = min_num_clusters + best_num_clusters_index\n",
    "# print(\"Silhouette Scores\")\n",
    "# for i in range(len(silhouette_scores)):\n",
    "#   print(i, \":\", silhouette_scores[i])\n",
    "\n",
    "# print(\"Optimal no. of clusters is\", best_num_clusters, \"with Silhouette score of\", silhouette_scores[best_num_clusters_index])\n",
    "\n",
    "\n",
    "# best_num_clusters_index = np.argmax(davies_bouldin_scores)\n",
    "# best_num_clusters = min_num_clusters + best_num_clusters_index \n",
    "# # print(\"Silhouette Scores\")\n",
    "# # for i in range(len(silhouette_scores)):\n",
    "# #   print(i, \":\", silhouette_scores[i])\n",
    "\n",
    "# print(\"Optimal no. of clusters is\", best_num_clusters, \"with David Bouldin score of\", davies_bouldin_scores[best_num_clusters_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "diVR8p5HvjUw",
    "outputId": "ce4c257b-322b-4eae-a2aa-60d1c8529eb9"
   },
   "outputs": [],
   "source": [
    "num_clusters = best_num_clusters\n",
    "model = KMeans(n_clusters=num_clusters)\n",
    "model.fit(features)\n",
    "labels = model.labels_\n",
    "labels = np.array(labels)\n",
    "\n",
    "for k in range(0, 7):\n",
    "  num = 0\n",
    "  print(\"\\nCluster \", k)\n",
    "  numImages = len(np.where(labels == k)[0])\n",
    "  print(numImages)\n",
    "  numRows = int(numImages / 10) + 1\n",
    "  numCols = 10\n",
    "  fig = plt.figure(k, figsize=(numCols * 5.0  , numRows * 5.0))\n",
    "  print(numRows, numCols)\n",
    "  for i in range(labels.shape[0]):\n",
    "    if(labels[i] == k):\n",
    "      imagePath = os.path.join(path + '/', str(images[i]))\n",
    "      img = image.load_img(imagePath)\n",
    "      plt.subplot(numRows, numCols, num+1)\n",
    "      plt.xticks([])\n",
    "      plt.yticks([])\n",
    "      plt.imshow(img)\n",
    "      num += 1\n",
    "  plt.show()\n",
    "# print(labels) # [4 3 3 ... 0 0 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Pre-trained ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras import applications\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import requests\n",
    "from PIL import Image\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from IPython.display import display, Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_width, img_height = 224, 224 #Can be changed to what we want\n",
    "\n",
    "train_data_dir = \"/data \"\n",
    "\n",
    "nb_train_samples = 801 #depends on number of samples we taking \n",
    "epochs = 50 #parameter to tune\n",
    "batch_size = 1 #parameter to tune\n",
    "\n",
    "def extract_features():\n",
    "    Itemcodes = []\n",
    "    datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "    model = applications.ResNet50(include_top=False, weights='imagenet')\n",
    "    generator = datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode=None,\n",
    "        shuffle=False)\n",
    "    for i in generator.filenames:\n",
    "        Itemcodes.append(i[(i.find(\"/\")+1):i.find(\".\")])\n",
    "    extracted_features = model.predict_generator(generator, nb_train_samples // batch_size)\n",
    "    extracted_features = extracted_features.reshape((801, 10035)) #1 is nb_train_samples #2 number of vectors we wish to represent as\n",
    "    np.save(open('./x.npy', 'wb'), extracted_features) #save as numpy array to save computational time\n",
    "    np.save(open('./x_ids.npy', 'wb'), np.array(Itemcodes)) #save as numpy array ro save computations time\n",
    "    \n",
    "a = datetime.now()\n",
    "extract_features()\n",
    "print(\"Time taken in feature extraction\", datetime.now()-a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articleIds = []\n",
    "for image_path in sample_image_paths:\n",
    "    articleIds.append(image_path.split('/')[-1].split('.')[0])\n",
    "    \n",
    "extracted_features = all_features.cpu().numpy()\n",
    "print(len(articleIds), extracted_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articleIds = np.array(articleIds)\n",
    "print(articleIds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #load all the data to be worked upon\n",
    "# extracted_features = np.load('/kaggle/working/x.npy')\n",
    "# Productids = np.load('/kaggle/working/xtids.npy')\n",
    "# #data_copy = data.copy()\n",
    "# #df_Productids = list(data['ProductId'])\n",
    "# Productids = list(Productids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Productids[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "men = pd.read_csv('articles.csv')\n",
    "print(men.columns)\n",
    "print(men.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "men['ImageURL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Productids = list(articleIds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get recommendation of similar items\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "def get_similar_products_cnn(product_id, num_results):\n",
    "    doc_id = Productids.index(product_id)\n",
    "    pairwise_dist = pairwise_distances(extracted_features, extracted_features[doc_id].reshape(1,-1))\n",
    "    indices = np.argsort(pairwise_dist.flatten())[0:num_results]\n",
    "    pdists  = np.sort(pairwise_dist.flatten())[0:num_results]\n",
    "    print(\"=\"*20, \"input product image\", \"=\"*20)\n",
    "    ip_row = men[['image_url','prod_name']].loc[men['article_id']==int(Productids[indices[0]])] #change for our use\n",
    "    #print(ip_row.head())\n",
    "    for indx, row in ip_row.iterrows():\n",
    "        display(Image(url=row['image_url'], width = 224, height = 224,embed=True)) #change for our use\n",
    "        print('Product Title: ', row['prod_name']) #change for our use\n",
    "    print(\"\\n\",\"=\"*20, \"Recommended products\", \"=\"*20)\n",
    "    for i in range(1,len(indices)):\n",
    "        rows = men[['image_url','prod_name']].loc[men['article_id']==int(Productids[indices[i]])] #change for our use\n",
    "        for indx, row in rows.iterrows():\n",
    "            display(Image(url=row['image_url'], width = 224, height = 224,embed=True)) #change for our use [width and height need to be ]\n",
    "            print('Product Title: ', row['prod_name']) #change for our use\n",
    "            print('Euclidean Distance from input image:', pdists[i])\n",
    "\n",
    "get_similar_products_cnn('0676255002', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar_recommendation(article_id, loop_number):\n",
    "    index = image_id.index(article_id)\n",
    "    pairwise_dist = pairwise_distances(features[index].reshape(1, -1), features)\n",
    "    indices = np.argsort(pairwise_dist.flatten())\n",
    "    print(indices)\n",
    "    print(pairwise_dist)\n",
    "    pdist = np.sort(pairwise_dist.flatten())\n",
    "    print(pdist)\n",
    "    for i in range(loop_number):\n",
    "        print(image_id[indices[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_similar_recommendation(\"0509937020.jpg\", 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairwise_dist = pairwise_distances(features[0].reshape(1, -1), features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.argsort(pairwise_dist.flatten())\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdist = np.sort(pairwise_dist.flatten())\n",
    "print(pdist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "image_features.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "mlp",
   "language": "python",
   "name": "mlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
